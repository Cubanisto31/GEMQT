# Framework d'ExpÃ©rimentation sur le RÃ©fÃ©rencement des Sources

Ce projet fournit un framework complet pour mener des expÃ©rimentations Ã  grande Ã©chelle sur la maniÃ¨re dont les modÃ¨les de langage (LLMs) et les moteurs de recherche citent leurs sources. Il est conÃ§u pour Ãªtre modulaire, extensible et automatisÃ©.

## Structure du Projet

```
.
â”œâ”€â”€ ğŸ“‚ experiment_results/
â”œâ”€â”€ ğŸ“‚ src/
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_client.py
â”‚   â”‚   â”œâ”€â”€ claude_client.py
â”‚   â”‚   â”œâ”€â”€ openai_client.py
â”‚   â”‚   â””â”€â”€ search_clients.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ runner.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

*Note*
On a rÃ©ussi grÃ¢ce Ã  claude code, Ã  faire tourner le code et l'on est en mesure d'analyser les rÃ©sultats. Toutefois il va falloir maintenant demandÃ© Ã  Claude code d'ajouter les rÃ©ponses complÃ¨tes aux requÃªtes (que je n'arrive pas encore Ã  trouver).

Il va falloir Ã©galement que l'on ajoute les autres modÃ¨les (il va falloir doucement se poser sur la question du prix de l'exp)

Il faudra aussi refaire une passe sur les requÃªtes testÃ©es

Il faut vÃ©rifier si le code tourne tous les jours comme prÃ©vu dans le Yaml ou pas (mais Ã§a m'Ã©tonnerait)
